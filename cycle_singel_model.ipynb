# -*- encoding: utf-8 -*-
'''
@File    :   Untitled-1
@Time    :   2020/09/14 14:55:53
@Author  :   Zhuo Yan 
'''

import pandas as pd
import numpy as np
from collections import Counter
from sklearn.model_selection import train_test_split
from bert_serving.client import BertClient
from sklearn.svm import SVC
from sklearn import svm
import synonyms
from sklearn.metrics.pairwise import cosine_similarity
from random import sample
import datetime
import time
import jieba
import random
from random import shuffle
from sklearn.externals import joblib
import os
import warnings
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score

warnings.filterwarnings("ignore")




def synonym_replacement(words, n):

    '''
        替换一个语句中的n个单词为其同义词
    '''
    f = open('Z:/download/stopword.txt',encoding="utf8")
    stop_words = list()
    for stop_word in f.readlines():
        stop_words.append(stop_word[:-1])
    new_words = words.copy()
    f.close()
    # 去除停用词
    random_word_list = list(set([word for word in words if word not in stop_words]))
    random.shuffle(random_word_list)
    num_replaced = 0  
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        # 有同义词的
        if len(synonyms) >= 1:
            synonym = random.choice(synonyms)
            # 按原顺序替换词
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break

    sentence = ' '.join(new_words)
    new_words = sentence.split(' ')

    return new_words

def get_synonyms(word):

    '''
        获取同义词
    '''
    return synonyms.nearby(word)[0]

def random_insertion(words, n):

    '''
        随机在语句中插入n个词
    '''
    new_words = words.copy()
    for _ in range(n):
        add_word(new_words)
    return new_words

def add_word(new_words):
    synonyms = []
    counter = 0    
    while len(synonyms) < 1:
        random_word = new_words[random.randint(0, len(new_words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = random.choice(synonyms)
    random_idx = random.randint(0, len(new_words)-1)
    new_words.insert(random_idx, random_synonym)

def random_swap(words, n):

    '''
         随机交换一个语句中的两个词n次
    '''
    new_words = words.copy()
    for _ in range(n):
        new_words = swap_word(new_words)
    return new_words

def swap_word(new_words):

    '''
        选取两个词交换的过程
    '''
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_2 == random_idx_1:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1
        if counter > 3:
            return new_words
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] 
    return new_words

def random_deletion(words, p):

    '''
        以概率p删除语句中的词
    '''
    if len(words) == 1:
        return words

    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]

    return new_words

def eda(sentence, alpha_sr=0.05, alpha_ri=0.05, alpha_rs=0.05, p_rd=0.05, num_aug=16):

    '''
        用四种增强方法处理一句话，增强出 num_aug 个语句，不包括原句
    '''
    import datetime
    seg_list = jieba.cut(sentence)
    seg_list = " ".join(seg_list)
    words = list(seg_list.split())
    num_words = len(words)
    augmented_sentences = []
    num_new_per_technique = int(num_aug/4)+1
    n_sr = max(1, int(alpha_sr * num_words))
    n_ri = max(1, int(alpha_ri * num_words))
    n_rs = max(1, int(alpha_rs * num_words))
    
    # 同义词替换sr
    for _ in range(num_new_per_technique):
        a_words = synonym_replacement(words, n_sr)
        augmented_sentences.append(''.join(a_words))
    
    # 随机插入ri
    for _ in range(num_new_per_technique):
        a_words = random_insertion(words, n_ri)
        augmented_sentences.append(''.join(a_words))
    
    # 随机交换rs
    for _ in range(num_new_per_technique):
        a_words = random_swap(words, n_rs)
        augmented_sentences.append(''.join(a_words))

    # 随机删除rd
    for _ in range(num_new_per_technique):
        a_words = random_deletion(words, p_rd)
        augmented_sentences.append(''.join(a_words))

    shuffle(augmented_sentences)

    if num_aug >= 1:
        augmented_sentences = augmented_sentences[:num_aug]
    else:
        keep_prob = num_aug / len(augmented_sentences)
        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]

    return augmented_sentences

def gen_eda(output_aug, alpha, num_aug,X_to_aug,y_to_aug):

    '''
        判断是否已经生成增强文件

        否，则进行增强，是，则直接读取

        输出：增强后的事件 X_aug、对应的标签 y_aug、增强时间 time_aug
    '''
    if 1:
        #  i = datetime.datetime.now()
        # print(f"\n=====增强中===== {i.month}/{i.day}  {i.hour}:{i.minute}\n ")
        st_aug = time.process_time()
    
        data = []
        for i in range(len(X_to_aug)):
            print(f"\r增强进度：{(float((i+1)/len(X_to_aug))):8.2%}   {i+1} / {len(X_to_aug) }",end=" ")
            num_aug_ = num_aug+1 # 包含原数据，如 5=4+1
            sentence = X_to_aug[i].replace('\n','').replace(' ','')
            aug_sentences = []
            num = 1
            # 第一次增强
            augment_ini = eda(sentence, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug)
    
            # 得到不重复的文本

            aug_sentences.append(sentence)
            # 去重
            for j in range(num_aug):
                if augment_ini[j] != sentence:
                    aug_sentences.append(augment_ini[j])
                    num += 1

            # 继续增强至 num_aug
            while num<num_aug_:
                augment_ini_ = eda(sentence, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug)
                for m in range(num_aug):
                    if augment_ini_[m] not in aug_sentences and augment_ini_[m] != sentence:
                        aug_sentences.append(augment_ini_[m])
                        num += 1

            # data_ 保存单个类型+事件，再保存到 data
            data_ = []
        
            for n in range(num_aug_):
                data_ = [y_to_aug[i],aug_sentences[n]]
                data.append(data_)       
    
        en_aug = time.process_time()
        time_aug = en_aug - st_aug
        final = pd.DataFrame(columns=["time_aug"],data=[[time_aug]])
        # final.to_csv(output_time,encoding='gbk')
        print(f"\n\n增强结束，用时：{time_aug} 秒\n")

        # 保存
        name = ['事件类型','简要经过']
        final = pd.DataFrame(columns=name,data=data)
        final.to_csv(output_aug,encoding='gbk')

        # 读取
        infor = pd.read_csv(output_aug,encoding='gb18030')
        X_aug_ = infor['简要经过'].tolist()
        y_aug = infor['事件类型'].tolist()

    return X_aug_,y_aug


if __name__ == "__main__":

    #  读取数据
    filename = 'Z:/data/big/incident4.0.csv'
    infor = pd.read_csv(filename,encoding='gb18030')
    contents = infor['简要经过'].tolist()
    types1_ = infor['事件类型1'].tolist()
    types2_ = infor['事件类型2'].tolist()
    types3_ = infor['事件类型3'].tolist()
    types1 = [[t1] for t1 in types1_]
    types2 = [[t2] for t2 in types2_]
    types3 = [[t3] for t3 in types3_]
    type_set = np.hstack((types1,types2,types3)).tolist()# 原读取列表中的nan值在 hstack 处理后变成字符串
    type_set_clear = [[tt for tt in t if tt!="nan"] for t in type_set]
    types_name_num = sorted(dict(Counter(types1_+types2_+types3_)).items(),key=lambda item:item[1],reverse=True)[1:]

    tt = datetime.datetime.now()
    print(f"=====模型拟合中===== {tt.month}/{tt.day}  {tt.hour}:{tt.minute}\n")

    model = []
    index_all = []
    prfs = []
    # 补充
    # types_name_num = [("机务维护_维修",130),("货物配载_装载",262)]
    for i in range(len(types_name_num)):
        print(f"--------正在处理第{i+1} / {len(types_name_num)}个， {types_name_num[i]} 事件--------\n")
        name = types_name_num[i][0]
        name_ = name.replace("/","_").replace("、","_")# 文件事件名
        filename = f"Z:/data/big/incident_pos_neg2.0/{name_}.csv"#读取的文件名

        if not os.path.exists(filename):#判断该类型正负例文件是否存在
            print(f"{name_}事件的文件不存在")
        else:
            try:
                infor = pd.read_csv(filename,encoding='gb18030')
            except UnicodeDecodeError:
                infor = pd.read_csv(filename)
            # 读取正负例事件经过，排除空格
            pos_ = infor['正例'].tolist()
            neg_ = infor['负例'].tolist()
            pos = [p.replace(" ","") for p in pos_ if p==p]
            neg = [n.replace(" ","") for n in neg_ if n==n]

            # X,y整理及测试集训练集划分
            X_text = pos + neg
            y = []
            for p in pos:
                y.append(1)
            for n in neg:
                y.append(0)
            test_size = 0.3
            X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=float(test_size), random_state=2021,stratify = y)

            # 增强
            output_aug = f"Z:/data/big/aug/{name_}.csv"
            if not os.path.exists(output_aug):
                # 100以上用百量级参数，以下用十量级参数
                if types_name_num[i][1] <= 100:
                    X_aug,y_aug = gen_eda(output_aug=f"Z:/data/big/aug/{name_}.csv",alpha=0.15,num_aug=20,X_to_aug=X_train,y_to_aug=y_train)
                else:
                    X_aug,y_aug = gen_eda(output_aug=f"Z:/data/big/aug/{name_}.csv",alpha=0.05,num_aug=8,X_to_aug=X_train,y_to_aug=y_train)
                np.savetxt(f"Z:/data/big/y_test/{name_}_test_y.csv", y_test, delimiter = ',')
            else:
                print("-----增强数据读取中----\n")
                infor = pd.read_csv(output_aug,encoding='gb18030')
                X_aug = infor['简要经过'].tolist()
                y_aug = infor['事件类型'].tolist()


            

            #训练集向量化
            train_vec_file = f"Z:/data/big/train_vec/{name_}_aug_v.csv"
            if not os.path.exists(train_vec_file):
                n = 1
                X_aug_v = []
                bc = BertClient()
                for _ in (X_aug):
                    x = bc.encode([_]).tolist()[0]
                    X_aug_v.append(x)
                    print(f"\r{name_}训练集向量化进度：{(float((n)/len(X_aug))):8.2%}   {n} / {len(X_aug) }",end=" ")
                    n+=1
                print("\n")
                np.savetxt(train_vec_file, X_aug_v, delimiter = ',') 
            else:
                f_t_v = open(train_vec_file,'rb')
                X_aug_v = np.loadtxt(f_t_v,delimiter=',',skiprows=0)
                X_aug_v = [X_.tolist() for X_ in X_aug_v ]
                f_t_v.close()

            #测试集向量化
            test_vec_file = f"Z:/data/big/test_vec/{name_}_test_v.csv"
            if not os.path.exists(test_vec_file):
                n = 1
                X_test_v = []
                bc = BertClient()
                for _ in (X_test):
                    x = bc.encode([_]).tolist()[0]
                    X_test_v.append(x)
                    print(f"\r{name_}测试集向量化进度：{(float((n)/len(X_test))):8.2%}   {n} / {len(X_test) }",end=" ")
                    n+=1
                print("\n")
                np.savetxt(test_vec_file, X_test_v, delimiter = ',') 
            else:
                f_t_v = open(test_vec_file,'rb')
                X_test_v = np.loadtxt(f_t_v,delimiter=',',skiprows=0)
                X_test_v = [X_.tolist() for X_ in X_test_v ]
                f_t_v.close()

            #训练
            svm_model = svm.SVC(gamma='scale', decision_function_shape='ovr',probability=True,class_weight='balanced')
            svm_model.fit(X_aug_v, y_aug)
            joblib.dump(svm_model,f"Z:/data/big/model/{name_}.pkl",compress=3)
            model.append(svm_model)


            # 真实标签
            pos_test_X = [X_test_v[i] for i in range(len(X_test_v)) if y_test[i] == 1]
            neg_test_X = [X_test_v[i] for i in range(len(X_test_v)) if y_test[i] == 0]
            pos_test_y = [y_test[i] for i in range(len(X_test_v)) if y_test[i] == 1]
            neg_test_y = [y_test[i] for i in range(len(X_test_v)) if y_test[i] == 0]

            pos_train_X = [X_aug_v[i] for i in range(len(X_aug_v)) if y_aug[i] == 1]
            neg_train_X = [X_aug_v[i] for i in range(len(X_aug_v)) if y_aug[i] == 0]
            pos_train_y = [y_aug[i] for i in range(len(X_aug_v)) if y_aug[i] == 1]
            neg_train_y = [y_aug[i] for i in range(len(X_aug_v)) if y_aug[i] == 0]

            tru_test_y = pos_test_y + neg_test_y
            tru_train_y = pos_train_y + neg_train_y

            # 预测标签
            pred_pos_test_y = [svm_model.predict([_]).tolist()[0] for _ in pos_test_X]
            pred_neg_test_y = [svm_model.predict([_]).tolist()[0] for _ in neg_test_X]
            pred_pos_train_y = [svm_model.predict([_]).tolist()[0] for _ in pos_train_X]
            pred_neg_train_y = [svm_model.predict([_]).tolist()[0] for _ in neg_train_X]

            pred_test_y = pred_pos_test_y + pred_neg_test_y
            pred_train_y = pred_pos_train_y + pred_neg_train_y

            # 正确率
            # 测试集中标签为1/测试集中标签为0/训练集中标签为1/训练集中标签为0
            as_test_1 = accuracy_score(pos_test_y,pred_pos_test_y)
            as_test_0 = accuracy_score(neg_test_y,pred_neg_test_y)
            as_train_1 = accuracy_score(pos_train_y,pred_pos_train_y)
            as_train_0 = accuracy_score(neg_train_y,pred_neg_train_y)

            as_test = [as_test_1,as_test_0]
            as_train = [as_train_1,as_train_0]

            # 权重
            test_weight = [n / len(y_test) for n in [sum(y_test),len(y_test)-sum(y_test)]]
            train_weight = [n / len(y_train) for n in [sum(y_train),len(y_train)-sum(y_train)]]

            as_test_total = sum([as_test[i]*test_weight[i] for i in range(len(as_test))])
            as_train_total = sum([as_train[i]*train_weight[i] for i in range(len(as_train))])

            # precision_recall_fscore_support（各类的精确率、召回率 、f值、各类数量）
            prfs_test_ = precision_recall_fscore_support(tru_test_y,pred_test_y,labels=[1,0])
            prfs_test = [p.tolist() for p in prfs_test_]
            prfs_train_ = precision_recall_fscore_support(tru_train_y,pred_train_y ,labels=[1,0])
            prfs_train = [p.tolist() for p in prfs_train_]

            # precision_recall_fscore_support_weighted（各类指标加权平均值）
            prfsw_test_ = list(precision_recall_fscore_support(tru_test_y,pred_test_y,average="weighted"))
            prfsw_test = [[p,"-"] for p in prfsw_test_]
            prfsw_train_ = list(precision_recall_fscore_support(tru_train_y,pred_train_y,average="weighted"))
            prfsw_train = [[p,"-"] for p in prfsw_train_]
            prfs.append([[[name,name]] + [[1,0]] + [as_test] + prfs_test + [[as_test_total,"-"]] + prfsw_test + [as_train] + prfs_train + [[as_train_total,"-"]] + prfsw_train])

    # 数据的标题索引
    prfs_index1 = ["test_as","test_p","test_r","test_f","test_s","test_total_as","test_total_p","test_total_r","test_total_f","test_total_s"]
    prfs_index2 = ["train_as","train_p","train_r","train_f","train_s","train_total_as","train_total_p","train_total_r","train_total_f","train_total_s"]
    prfs_index = ["事件类型"] + ["label"] + prfs_index1 + prfs_index2

    # 指标放一起，一个类型对应index_all的一个元素，一个元素嵌套有22个指标元素
    for i in range(len(prfs)):
        if len(types_name_num)==1:
            index_all = prfs[0]
        else:
            if i==0:
                index_all = np.hstack((prfs[i][0],prfs[i+1][0])).tolist()
            if 0<i<len(prfs)-1:
                index_all = np.hstack((index_all,prfs[i+1][0])).tolist()
        
    # 保存
    output_prfs = pd.DataFrame(index=prfs_index,data=index_all)
    output_prfs.T.to_csv(f"Z:/data/big/index/index2.csv",encoding='gbk')

    time = datetime.datetime.now()
    print(f"=====结束===== {time.month}/{time.day}  {time.hour}:{time.minute}\n")












